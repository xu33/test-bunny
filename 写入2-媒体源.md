⸻

媒体源（Media Sources）

媒体源（Media Sources）提供了向 Output 写入媒体数据的接口。不同的媒体源抽象级别不同，适用于不同的使用场景。 ￼

要了解如何用媒体源为输出添加轨道，请参见 “Writing media files” 章节。 ￼

大多数媒体源都遵循一种通用的使用方式：

await mediaSource.add(...);


⸻

关闭媒体源（Closing Sources）

当你确定不再向某个媒体源写入更多数据时，最好尽快手动调用其 close() 方法：

mediaSource.close();

虽然技术上在 Output.finalize() 时会自动关闭所有源，但如果你的 Output 有多个轨道且它们完成写入的时间不同（例如先写完音频后再写视频），提前关闭那些已完成的源能提升性能、减少内存占用。因为 Output 会据此更早知道某些轨道不再有数据进来，从而减少等待或缓存。 ￼

因此，作为一个良好实践，一旦用完就关闭媒体源。 ￼

⸻

背压（Backpressure）

媒体源是 Output 管线向上（到你的应用逻辑）传导背压的通路。也就是说，如果编码器或写入目标（如 StreamTarget 的 WritableStream）无法及时处理数据，媒体源会让 add() 方法返回一个不会立即 resolve 的 Promise，从而“暂停”你的写入流程，直到系统准备好继续为止。 ￼

具体地说，所有带 add 方法的媒体源，其 add(...) 都会返回一个 Promise<void>：

mediaSource.add(...);  // => Promise<void>

通常这个 Promise 会立即 resolve。但如果下游处理能力跟不上，它就会被挂起直到可以继续。通过 await mediaSource.add(...)，你的写入逻辑就会自然而然地响应背压。 ￼

要注意的是，不要像下面这样写：

// 错误写法 — 不等待 add 完成
while (notDone) {
  mediaSource.add(...);
}

正确方式是：

while (notDone) {
  await mediaSource.add(...);
}

这样才能使得背压机制生效，避免写入速度过快淹没写入端。 ￼

⸻

视频类媒体源（Video Sources）

所有可用于视频轨道的源都继承自抽象的 VideoSource 类。 ￼

下面是几种常用的视频源：

VideoSampleSource

这个源接收「视频样本（video samples）」，内部负责对样本进行编码，然后把编码后的数据交给 Output。 ￼

使用示例：

import { VideoSampleSource } from 'mediabunny';

const sampleSource = new VideoSampleSource({
  codec: 'avc',
  bitrate: 1e6,
});
await sampleSource.add(videoSample);
videoSample.close();  // 若不再使用这个样本
// 如果你想强制这个样本作为关键帧 (key frame)：
await sampleSource.add(videoSample, { keyFrame: true });

CanvasSource

如果你的场景是：不断绘制在一个 <canvas> 上，每帧截图写入文件，那么 CanvasSource 是很方便的封装。 ￼

使用示例：

import { CanvasSource, QUALITY_MEDIUM } from 'mediabunny';

const canvasSource = new CanvasSource(canvasElement, {
  codec: 'av1',
  bitrate: QUALITY_MEDIUM,
});
await canvasSource.add(0.0, 0.1);
await canvasSource.add(0.1, 0.1);
await canvasSource.add(0.2, 0.1);
// 你也可以强制某帧作为关键帧：
await canvasSource.add(0.3, 0.1, { keyFrame: true });

这里的 add(timestamp, duration, [options]) 接受时间戳（单位为秒）和持续时间，然后内部负责编码和写入。 ￼

MediaStreamVideoTrackSource

如果你要把一个实时视频流（例如摄像头、屏幕录制）写入输出文件，就可以使用这个源。它与 Web 的 Media Capture & Streams API 集成。 ￼

例如：

import { MediaStreamVideoTrackSource } from 'mediabunny';

const stream = await navigator.mediaDevices.getDisplayMedia({ video: true });
const videoTrack = stream.getVideoTracks()[0];
const videoTrackSource = new MediaStreamVideoTrackSource(videoTrack, {
  codec: 'vp9',
  bitrate: 1e7,
});

一些要点：
	•	一旦你对 Output 调用了 start()，数据就会自动从 videoTrack 捕获并写出。你不需要手动调用 add。 ￼
	•	如果不再需要该视频源，别忘了调用 videoTrack.stop()，以释放资源。 ￼
	•	此源内部可能有异步错误，你应该用 errorPromise 捕获这些错误。 ￼
	•	如果这是 Output 中唯一的 MediaStreamTrack 源，那么它输出的首帧时间戳就是 0。如果有多个轨道，所有轨道会以它们各自的最早样本时间对齐，使得它们同步。 ￼

EncodedVideoPacketSource

这是最底层的视频源之一：你自己负责编码／生成已编码的视频包（encoded packets），然后把它们“打包”进输出。用于高级用户或特定场景。 ￼

示例：

import { EncodedVideoPacketSource } from 'mediabunny';

const packetSource = new EncodedVideoPacketSource('vp9');
await packetSource.add(packet1);
await packetSource.add(packet2);

要注意：
	•	你必须以解码顺序（decode order）调用 add，不能只按照显示顺序（presentation order）乱写。 ￼
	•	在第一次 add 调用时，你还需要带上元数据（metadata），用于向 Output 描述该视频数据的形状、编码格式等。这个 metadata 格式与 WebCodecs API 中的 EncodedVideoChunkMetadata 一致。 ￼

metadata 至少要包括 codec、codedWidth、codedHeight；可能还要包括 colorSpace、description 等字段（视具体 codec 而定） ￼

关于 B 帧（B-frames）：

视频中可能存在 B 帧（双向预测帧），它在解码时需要同时参考之前帧和之后帧。假设你有：
	•	帧1：I 帧 (0.0 s)
	•	帧2：B 帧 (0.1 s)
	•	帧3：P 帧 (0.2 s)

它们的解码顺序可能是：帧1 → 帧3 → 帧2。 ￼

虽然 EncodedVideoPacket 只包含展示时间戳 (presentation timestamp)，但你添加包的顺序决定的是解码顺序，所以你必须按解码顺序调用 add。例如：

await packetSource.add(packetForFrame1);
await packetSource.add(packetForFrame3);
await packetSource.add(packetForFrame2);

至于时间戳排序的限制，有一条规则需求：

你所添加的包的时间戳 不能 比最近一次你添加的关键帧（key frame）之前的最大时间戳还要小。 ￼

换句话说，在一个关键帧之后，你不能再回头插入时间戳低于之前已使用最大时间戳的数据包。原文中举了一些合法与非法示例说明这个约束。 ￼

⸻

音频类媒体源（Audio Sources）

这些源用于向音频轨道写入数据，所有音频源都继承自抽象的 AudioSource 类。 ￼

下面是几种常见音频源：

AudioSampleSource

这个源接受「音频样本（audio samples）」，内部负责编码并交给 Output。 ￼

示例：

import { AudioSampleSource } from 'mediabunny';

const sampleSource = new AudioSampleSource({
  codec: 'aac',
  bitrate: 128e3,
});
await sampleSource.add(audioSample);
audioSample.close();  // 若不再使用这个样本

AudioBufferSource

这个源直接接收 AudioBuffer（Web Audio API 中的音频缓冲区），比较适合与你在浏览器中使用 Web Audio 的场景对接。 ￼

使用示例：

import { AudioBufferSource, QUALITY_MEDIUM } from 'mediabunny';

const bufferSource = new AudioBufferSource({
  codec: 'opus',
  bitrate: QUALITY_MEDIUM,
});
await bufferSource.add(audioBuffer1);
await bufferSource.add(audioBuffer2);
await bufferSource.add(audioBuffer3);

第一个 AudioBuffer 会在 timestamp = 0 时播放，之后的缓冲按顺序接在后面。 ￼

MediaStreamAudioTrackSource

这个源用于将实时音频（如麦克风输入、系统音频流）写入输出文件，非常适合与 Media Capture & Streams API 结合使用。 ￼

示例：

import { MediaStreamAudioTrackSource } from 'mediabunny';

const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
const audioTrack = stream.getAudioTracks()[0];
const audioTrackSource = new MediaStreamAudioTrackSource(audioTrack, {
  codec: 'opus',
  bitrate: 128e3,
});

要点：
	•	一旦你调用 output.start()，音频数据会自动录入输出，无需你手动调用 add。 ￼
	•	完成后要调用 audioTrack.stop() 释放麦克风等资源。 ￼
	•	内部可能发生错误，应通过 errorPromise 处理异常。 ￼
	•	与视频源一样：如果这是 Output 中唯一的 MediaStreamTrack 源，那么它的第一条样本时间戳就是 0；若混合多个源，各轨道会对齐以最早时间戳为 0，以保持同步。 ￼

EncodedAudioPacketSource

这是最底层的音频源，你自己负责编码／生成已编码的音频包，然后把它们喂给输出。适合高级用途或跳过解码／重编码场景。 ￼

示例：

import { EncodedAudioPacketSource } from 'mediabunny';

const packetSource = new EncodedAudioPacketSource('aac');
await packetSource.add(packet);

首次 add 时你必须提供元数据（metadata），其格式与 WebCodecs 中的 EncodedAudioChunkMetadata 一致。至少要包含 codec、numberOfChannels、sampleRate；某些 codec 还需要 description 字段。 ￼

⸻

字幕类媒体源（Subtitle Sources）

这些源用于向字幕轨道（subtitle track）写入数据，均继承自抽象类 SubtitleSource。 ￼

下面是目前支持的一种：

TextSubtitleSource

这个源接收纯文本格式的字幕（比如 WebVTT），并将字幕 Cue 写入输出。 ￼

示例：

import { TextSubtitleSource } from 'mediabunny';

const textSource = new TextSubtitleSource('webvtt');

const text =
`WEBVTT

00:00:00.000 --> 00:00:02.000
This is your last chance.

00:00:02.500 --> 00:00:04.000
After this, there is no turning back.

...`;
await textSource.add(text);

如果你一次性把整段字幕文本加进去，最好紧接着 textSource.close()。 ￼

你也可以分块逐条添加 Cue：

await textSource.add('WEBVTT\n\n');
await textSource.add('00:00:00.000 --> 00:00:02.000\n Hello there!\n\n');
await textSource.add('00:00:02.500 --> 00:00:04.000\n Chunky chunks.\n\n');

要注意：
	•	每个字幕 Cue 必须整体包含在某一个 chunk 内，不能拆分成多个 chunk。 ￼
	•	WebVTT 的前导头（preamble）必须作为最前的一块，一次性写入（不能分散写）。 ￼

⸻

如果你愿意，我也可以出一份这篇的 注释版中文（加上解释、注意事项、示意图等），或是把你特别关心的那一类媒体源重点翻译／扩展。要吗？